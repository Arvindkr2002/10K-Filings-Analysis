{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uT_8wDRRuems",
        "tayAwAcIu1tp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install sec_downloader transformers streamlit streamlit_modal"
      ],
      "metadata": {
        "id": "R5pNr3UBJlw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import dependencies\n",
        "from sec_downloader import Downloader\n",
        "from sec_downloader.types import RequestedFilings\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SDwj1wKtpNFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions\n",
        "\n",
        "summarizer = pipeline(\"summarization\",model='facebook/bart-large-cnn')\n",
        "analyser=pipeline('sentiment-analysis',model='ProsusAI/finbert',top_k=3)\n",
        "dl = Downloader('MSFT', \"arvind.kr.200202@gmail.com\")\n",
        "\n",
        "def extract_text(url, yr, ticker):\n",
        "    '''\n",
        "    given an URL it Extract text from it using beautiful soup\n",
        "    '''\n",
        "    soup = BeautifulSoup(dl.download_filing(url=url).decode(), 'html.parser')\n",
        "\n",
        "    #create directory to store txt files\n",
        "    if not os.path.exists(ticker):\n",
        "        os.makedirs(ticker)\n",
        "\n",
        "    # Extract text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Save text to a .txt file\n",
        "    with open(f\"{ticker}/{yr}.txt\", \"w\") as file:\n",
        "        file.write(text)\n",
        "        file.write(\"\\n<EOF>\")  # Write EOF marker\n",
        "\n",
        "def split_content(max_part_words,overlap_words,content):\n",
        "  '''\n",
        "  Given a content, It split it into parts each having 'max_part_words' words and adjacent parts will have 'overlap_words' words overlapping between them\n",
        "  '''\n",
        "  # Use regex to split the content into words, also cleans the content\n",
        "  words = re.findall(r'\\w+', content)\n",
        "  parts = []\n",
        "  part_content = \"\"\n",
        "  # Iterate over words and split into parts with overlap\n",
        "  for i, word in enumerate(words):\n",
        "      # If adding the current word exceeds the max_part_words, append part_content to parts\n",
        "      if len(part_content.split()) + 1 > max_part_words:\n",
        "          parts.append(part_content)\n",
        "          # Adjust the start index for the next part considering overlap\n",
        "          start_index = max(0, i - overlap_words)\n",
        "          # Reset part_content with overlapping words\n",
        "          part_content = \" \".join(words[start_index:i+1]) + \" \"\n",
        "      else:\n",
        "          part_content += word + \" \"\n",
        "  # Append the last part_content to parts\n",
        "  if part_content:\n",
        "      parts.append(part_content)\n",
        "\n",
        "  return parts\n",
        "\n",
        "def extract_score(given_label,data):\n",
        "  '''\n",
        "  Given a dictionary of label and scores, It return score cooresponding to the given_label\n",
        "  '''\n",
        "  score = None\n",
        "  for item in data[0]: #iterate over all labels\n",
        "      if item['label'] == given_label: #match found\n",
        "          score = item['score']\n",
        "          break\n",
        "  return score\n",
        "\n",
        "def change_keys(dic):\n",
        "  #changing key with all 4 digits in the yr\n",
        "  l=list(dic.keys())\n",
        "  mod_dic={}\n",
        "  for i in l:\n",
        "    if int(i)<25:\n",
        "      mod_dic['20'+i]=dic[i]\n",
        "    else:\n",
        "      mod_dic['19'+i]=dic[i]\n",
        "  return mod_dic\n",
        "\n",
        "def extract_summary_and_sentiment(ticker):\n",
        "  '''\n",
        "  Given a ticker, it return a combined summary of all important segements, accross different years.\n",
        "  To find important segments, the content is split into chunks and whichever chunk has a neutral score <0.5 is considered important\n",
        "  Since the content is huge, all lot of it would tend to be neutral, So we modify sentiment score measurement to take averge only over important segements and leave out neutral segments\n",
        "  '''\n",
        "  dict_summary={}\n",
        "  dict_senti={}\n",
        "  #iterate over all files of the company\n",
        "  for file in os.listdir('/content/'+ticker):\n",
        "        #extract content from files\n",
        "        with open((os.path.join('/content/'+ticker, file)),'r') as fi:\n",
        "                  content = fi.read()\n",
        "        summaries=content\n",
        "        a=400\n",
        "        b=50\n",
        "        parts=split_content(a,b,summaries) #split it into chunks of suitable size\n",
        "        summaries = ''\n",
        "        sentiment=[]\n",
        "        for i, part in enumerate(parts):\n",
        "              try:\n",
        "                senti = analyser(part) #find sentiment score\n",
        "                if extract_score('neutral',senti)<0.5: #filtering out important segemnts\n",
        "                  summaries+=summarizer(part,max_length=80,min_length=40)[0]['summary_text'] #finding sumary\n",
        "                  #appending sentiment scores\n",
        "                  sentiment.append(extract_score('positive',senti))\n",
        "                  sentiment.append(-1*extract_score('negative',senti))\n",
        "              except:\n",
        "                continue\n",
        "        #appending to dictionary with key as last two digits of year\n",
        "        if sentiment:\n",
        "          dict_senti[file[:2]]=sum(sentiment)/(len(sentiment)//2) #calculating overall sentiment score, giving equal weightage to all segments\n",
        "        else: dict_senti[file[:2]]=0\n",
        "        dict_summary[file[:2]]=summaries\n",
        "\n",
        "\n",
        "  dict_summary=change_keys(dict_summary)\n",
        "  dict_senti=change_keys(dict_senti)\n",
        "  return dict_summary, dict_senti\n",
        "\n",
        "def combine_yrs(dic):\n",
        "  # combining summary of all the years\n",
        "  summary=''\n",
        "  for i in dic:\n",
        "    if dic[i]:\n",
        "      summary+= \" \"+ i+' : ('+dic[i]+\")\"\n",
        "  return summary\n",
        "\n",
        "\n",
        "def extract_positives_and_negatives(summary):\n",
        "  '''\n",
        "  Given the combined summary, it further summarises it, and return the positves and negatives seperately\n",
        "  '''\n",
        "  a=400\n",
        "  b=50\n",
        "  parts=split_content(a,b,summary)  #split it into chunks of suitable size\n",
        "  positives = ''\n",
        "  negatives=''\n",
        "  for i, part in enumerate(parts):\n",
        "        try:\n",
        "          senti = analyser(part) #find sentiment score\n",
        "          if extract_score('neutral',senti)<0.5: #filtering out important segemnts\n",
        "            if extract_score('positive',senti)>extract_score('negative',senti): #checking for positive\n",
        "              positives+=summarizer(part,max_length=80,min_length=40)[0]['summary_text']\n",
        "            else:\n",
        "              negatives+=summarizer(part,max_length=80,min_length=40)[0]['summary_text']\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "  return positives, negatives\n",
        "\n"
      ],
      "metadata": {
        "id": "iMYmRpORGuv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download files"
      ],
      "metadata": {
        "id": "uT_8wDRRuems"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ticker in ['AAPL','MSFT']:\n",
        "  metadatas = dl.get_filing_metadatas(\n",
        "      RequestedFilings(ticker_or_cik=ticker, form_type=\"10-K\", limit=29)\n",
        "  )\n",
        "  for i in metadatas:\n",
        "    try:\n",
        "      extract_text(i.primary_doc_url,i.accession_number.split('-')[1],ticker)\n",
        "    except:\n",
        "      continue"
      ],
      "metadata": {
        "id": "dMD9lxXCRLId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarise, combine and get insights"
      ],
      "metadata": {
        "id": "Uql51Py6uPyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ticker in ['AAPL','MSFT']:\n",
        "  globals()[f'{ticker}_yrly_summary'],globals()[f'{ticker}_yrly_sentiment']=extract_summary_and_sentiment(ticker)\n",
        "  globals()[f'{ticker}_overall_summary']=combine_yrs(globals()[f'{ticker}_yrly_summary'])\n",
        "  globals()[f'{ticker}_positives'],globals()[f'{ticker}_negatives']=extract_positives_and_negatives(globals()[f'{ticker}_overall_summary'])"
      ],
      "metadata": {
        "id": "arqI_alL-DM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store text file to access in app.py\n",
        "with open(f\"AAPL_positives.txt\", \"w\") as file:\n",
        "        file.write(AAPL_positives)\n",
        "with open(f\"AAPL_negatives.txt\", \"w\") as file:\n",
        "        file.write(AAPL_negatives)\n",
        "with open(f\"MSFT_positives.txt\", \"w\") as file:\n",
        "        file.write(MSFT_positives)\n",
        "with open(f\"MSFT_negatives.txt\", \"w\") as file:\n",
        "        file.write(MSFT_negatives)"
      ],
      "metadata": {
        "id": "Q2iQQRUraO-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store CSV file to access in app.py\n",
        "df=pd.DataFrame(AAPL_yrly_sentiment.items(), columns=['Year', 'Value'])\n",
        "df=df.sort_values(by='Year')\n",
        "df.to_csv('AAPL_yrly_sentiment.csv')\n",
        "df=pd.DataFrame(MSFT_yrly_sentiment.items(), columns=['Year', 'Value'])\n",
        "df=df.sort_values(by='Year')\n",
        "df.to_csv('MSFT_yrly_sentiment.csv')"
      ],
      "metadata": {
        "id": "amz9JBDWXcgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App"
      ],
      "metadata": {
        "id": "tayAwAcIu1tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PouzCg7Huo0C",
        "outputId": "11031f9c-a649-4122-b7ca-963f68aecee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.227.76.194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "IRQV3nIcfP6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHK3OSF1wE8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}